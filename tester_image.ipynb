{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SIDDHESHWAR DUBEY\\AppData\\Local\\Temp\\ipykernel_14092\\1037643380.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"gender_detection_model.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GenderCNN(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc1): Linear(in_features=200704, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the model architecture\n",
    "class GenderCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GenderCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 56 * 56, 512)\n",
    "        self.fc2 = nn.Linear(512, 2)  # 2 classes: male and female\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 56 * 56)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Load the model\n",
    "model = GenderCNN()\n",
    "model.load_state_dict(torch.load(\"gender_detection_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the Haar Cascade for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "def detect_faces(image_path):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    # Return the image and detected faces\n",
    "    return image, faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define transforms\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def preprocess_face(image, face):\n",
    "    x, y, w, h = face\n",
    "    face_image = image[y:y+h, x:x+w]  # Crop the face\n",
    "    face_image = cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "    face_image = Image.fromarray(face_image)  # Convert to PIL image\n",
    "    face_image = test_transform(face_image).unsqueeze(0).to(device)  # Apply transforms\n",
    "    return face_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_gender(face_image, model):\n",
    "    with torch.no_grad():\n",
    "        output = model(face_image)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        return \"Female\" if predicted.item() == 0 else \"Male\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face 1: Predicted Gender = Female\n"
     ]
    }
   ],
   "source": [
    "# Load and detect faces in the image\n",
    "image_path = \"C:/Users/SIDDHESHWAR DUBEY/Pictures/20210525_185048.jpg\"\n",
    "image, faces = detect_faces(image_path)\n",
    "\n",
    "# Process each detected face\n",
    "for i, (x, y, w, h) in enumerate(faces):\n",
    "    # Preprocess the face\n",
    "    face_image = preprocess_face(image, (x, y, w, h))\n",
    "\n",
    "    # Predict gender\n",
    "    gender = predict_gender(face_image, model)\n",
    "    print(f\"Face {i + 1}: Predicted Gender = {gender}\")\n",
    "\n",
    "    # Draw a rectangle around the face and label it\n",
    "    cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    cv2.putText(image, gender, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "# Display the result\n",
    "cv2.imshow(\"Detected Faces\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
